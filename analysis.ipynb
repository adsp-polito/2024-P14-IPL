{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Project",
   "id": "66c7d8995068d97a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ## Product Rating",
   "id": "f76a12932189a8d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "\n",
    "# Carica il primo dataset\n",
    "dataset1 = pd.read_csv(\"Tiny Eco Wonderflow3.csv\", sep=\";\", on_bad_lines=\"skip\")  # Prova con \";\" come separatore\n",
    "dataset1.columns = dataset1.columns.str.strip()\n",
    "\n",
    "# Estrai la colonna 'text' dal primo dataset\n",
    "texts1 = dataset1['text'].dropna()\n",
    "texts1 = texts1[texts1.str.strip() != '']\n",
    "\n",
    "# Rimuovi gli spazi all'inizio e alla fine del testo, e le nuove righe extra nel primo dataset\n",
    "texts1_cleaned = texts1.str.replace(r'\\s+', ' ', regex=True)  # Rimuove spazi extra (compresi \\n e \\t)\n",
    "texts1 = texts1_cleaned.str.strip()  # Rimuove gli spazi ai bordi\n",
    "\n",
    "###\n",
    "\n",
    "# Carica il secondo dataset\n",
    "dataset2 = pd.read_csv(\"Tiny Eco Digimind.csv\", sep=\";\", on_bad_lines=\"skip\") \n",
    "dataset2.columns = dataset2.columns.str.strip()\n",
    "\n",
    "# Estrai e concatena 'Title' e 'Detail' nel secondo dataset\n",
    "texts2 = dataset2['Title'].fillna('') + ' ' + dataset2['Detail'].fillna('')\n",
    "texts2 = texts2[texts2.str.strip() != '']\n",
    "\n",
    "# Unisci i due datasets di testo (puoi usare un'unica variabile per combinarli)\n",
    "texts= pd.concat([texts1, texts2], ignore_index=True)\n",
    "\n",
    "#texts è solo il primo \n",
    "texts=texts1\n",
    "\n",
    "\n",
    "# Funzione per ottenere il sentiment per ogni testo\n",
    "def get_sentiment(text, true_sentiment):\n",
    "    # Tokenizza il testo\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Invia il testo attraverso il modello\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Ottieni le probabilità di sentiment (5 classi da 0 a 4)\n",
    "    sentiment_scores = outputs.logits\n",
    "    sentiment = torch.argmax(sentiment_scores, dim=1).item()  # Ottieni la classe con la probabilità più alta\n",
    "\n",
    "    sentiment = sentiment + 1  # Mappa da 0-4 a 1-5\n",
    "\n",
    "    return {'text': text, 'sentiment': sentiment, 'trueSentiment': true_sentiment}\n",
    "\n",
    "# Applica la funzione di sentiment analysis a tutti i testi\n",
    "sentiments = [get_sentiment(text, true_sentiment) for text, true_sentiment in zip(texts, dataset1['feedbackRating'])]\n",
    "\n",
    "print(sentiments)\n",
    "\n",
    "# Converte i risultati in un DataFrame\n",
    "#sentiments_df = pd.DataFrame(sentiments)\n",
    "\n",
    "# Calcola l'accuratezza: confronta sentiment e trueSentiment\n",
    "#correct_predictions = (sentiments_df['sentiment'] == sentiments_df['trueSentiment']).sum()\n",
    "#accuracy = correct_predictions / len(sentiments_df) * 100\n",
    "\n",
    "# Calcola l'accuracy off-by-one: quando la differenza tra sentiment e trueSentiment è 1\n",
    "#accuracy_off_by_one = (abs(sentiments_df['trueSentiment'] - sentiments_df['sentiment']) <= 1).sum()\n",
    "#accuracy_off_by_one_percent = accuracy_off_by_one / len(sentiments_df) * 100\n",
    "\n",
    "\n",
    "# Stampa l'accuratezza e l'accuracy-1\n",
    "#print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "#print(f\"Accuracy off-by-one: {accuracy_off_by_one_percent:.2f}%\")\n",
    "\n",
    "#accuracy 30%, off by one 65.4%"
   ],
   "id": "10eaea7804c055f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sentiment Classifier ",
   "id": "caf187edced0049f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\")\n",
    "\n",
    "#questo print ci fa sapere queli sono le label \n",
    "#print(model.config.id2label)\n",
    "\n",
    "# Carica il secondo dataset\n",
    "dataset2 = pd.read_csv(\"Tiny Eco Digimind.csv\", sep=\";\", on_bad_lines=\"skip\")\n",
    "dataset2.columns = dataset2.columns.str.strip()\n",
    "\n",
    "texts2= texts2.str.replace(r'\\s+', ' ', regex=True)  # Rimuove spazi extra (compresi \\n e \\t)\n",
    "texts2 = texts2.str.strip().str.replace('\\n', ' ', regex=True)\n",
    "\n",
    "trueSentiment2 = dataset2['Sentiment'].str.strip()\n",
    "\n",
    "def get_sentiment(text, true_sentiment):\n",
    "    # Tokenizza il testo\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Invia il testo attraverso il modello\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Ottieni le probabilità di sentiment (5 classi da 0 a 4)\n",
    "    sentiment_scores = outputs.logits\n",
    "    sentiment_numeric = torch.argmax(sentiment_scores, dim=1).item()  # Ottieni la classe con la probabilità più alta\n",
    "\n",
    "    # Mappa numeri a etichette\n",
    "    sentiment_map = {\n",
    "        0: \"Positive\",\n",
    "        1: \"No sentiment\",\n",
    "        2: \"Negative\"\n",
    "    }\n",
    "    sentiment = sentiment_map.get(sentiment_numeric, \"Unknown\")  # Default a 'Unknown' per valori imprevisti\n",
    "\n",
    "    return {'sentiment': sentiment, 'trueSentiment': true_sentiment}\n",
    "\n",
    "\n",
    "# Applica la funzione di sentiment analysis a tutti i testi\n",
    "sentiments2 = [get_sentiment(text2, true_sentiment) for text2, true_sentiment in zip(texts2, trueSentiment2)]\n",
    "\n",
    "# Estrai le predizioni e i sentimenti veri\n",
    "predicted_sentiments = [item['sentiment'] for item in sentiments2]\n",
    "true_sentiments = list(trueSentiment2)\n",
    "\n",
    "# Calcola l'accuracy\n",
    "accuracy = accuracy_score(true_sentiments, predicted_sentiments)\n",
    "\n",
    "# Stampa il risultato\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n"
   ],
   "id": "8baae40e1aeb36b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This model has an accuracy of 35.01%, now we will evaluate another sentiment classifier and see which one works better.",
   "id": "ed727dded58c4b5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Verifica se la GPU è disponibile\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"citizenlab/twitter-xlm-roberta-base-sentiment-finetunned\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"citizenlab/twitter-xlm-roberta-base-sentiment-finetunned\")\n",
    "\n",
    "#print(model.config.id2label)\n",
    "\n",
    "\n",
    "# Carica il secondo dataset\n",
    "dataset2 = pd.read_csv(\"Tiny Eco Digimind.csv\", sep=\";\", on_bad_lines=\"skip\")\n",
    "dataset2.columns = dataset2.columns.str.strip()\n",
    "\n",
    "texts2= texts2.str.replace(r'\\s+', ' ', regex=True)  # Rimuove spazi extra (compresi \\n e \\t)\n",
    "texts2 = texts2.str.strip().str.replace('\\n', ' ', regex=True)\n",
    "\n",
    "trueSentiment2 = dataset2['Sentiment'].str.strip()\n",
    "\n",
    "def get_sentiment(text, true_sentiment):\n",
    "    # Tokenizza il testo\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Invia il testo attraverso il modello\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Ottieni le probabilità di sentiment (5 classi da 0 a 4)\n",
    "    sentiment_scores = outputs.logits\n",
    "    sentiment_numeric = torch.argmax(sentiment_scores, dim=1).item()  # Ottieni la classe con la probabilità più alta\n",
    "\n",
    "    # Mappa numeri a etichette\n",
    "    sentiment_map = {\n",
    "        0: \"Negative\",\n",
    "        1: \"No sentiment\",\n",
    "        2: \"Positive\"\n",
    "    }\n",
    "    sentiment = sentiment_map.get(sentiment_numeric, \"Unknown\")  # Default a 'Unknown' per valori imprevisti\n",
    "\n",
    "    return {'sentiment': sentiment, 'trueSentiment': true_sentiment}\n",
    "\n",
    "# Applica la funzione di sentiment analysis a tutti i testi\n",
    "sentiments3 = [get_sentiment(text2, true_sentiment) for text2, true_sentiment in zip(texts2, trueSentiment2)]\n",
    "\n",
    "print(sentiments3)\n",
    "\n",
    "# Estrai le predizioni e i sentimenti veri\n",
    "predicted_sentiments = [item['sentiment'] for item in sentiments3]\n",
    "true_sentiments = list(trueSentiment2)\n",
    "\n",
    "# Calcola l'accuracy\n",
    "accuracy = accuracy_score(true_sentiments, predicted_sentiments)\n",
    "\n",
    "# Stampa il risultato\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ],
   "id": "cbae04f72c52b6b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This model has an accuracy of 59.97%",
   "id": "e2460980d4d23a09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "82502f6957ddf10d"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-23T17:29:27.766699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "\n",
    "# Verifica se la GPU è disponibile\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Carica il tokenizer e il modello di traduzione (Italiano -> Inglese)\n",
    "translation_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-it-en\", use_fast=False)\n",
    "translation_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-it-en\").to(device)\n",
    "\n",
    "# Carica il dataset\n",
    "dataset2 = pd.read_csv(\"Tiny Eco Digimind.csv\", sep=\";\", on_bad_lines=\"skip\")\n",
    "dataset2.columns = dataset2.columns.str.strip()\n",
    "\n",
    "# Prepara i testi\n",
    "texts2 = dataset2['Title'].fillna('') + ' ' + dataset2['Detail'].fillna('')\n",
    "texts2 = texts2.str.replace(r'\\s+', ' ', regex=True).str.strip().str.replace('\\n', ' ', regex=True)\n",
    "\n",
    "# Funzione per la traduzione\n",
    "def translate_text_it_to_en(text):\n",
    "    # Tokenizza il testo per la traduzione\n",
    "    inputs = translation_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "    # Invia il testo al modello di traduzione\n",
    "    with torch.no_grad():\n",
    "        translated_ids = translation_model.generate(**inputs)\n",
    "\n",
    "    # Decodifica la traduzione (da ID a testo)\n",
    "    translated_text = translation_tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Tradurre tutte le frasi\n",
    "translated_texts = [translate_text_it_to_en(text) for text in texts2]\n",
    "\n",
    "# Crea un DataFrame con le frasi originali e tradotte\n",
    "translation_df = pd.DataFrame({\n",
    "    'Original': texts2,\n",
    "    'Translated': translated_texts\n",
    "})\n",
    "\n",
    "# Stampa la matrice delle frasi originali e tradotte\n",
    "print(translation_df)\n"
   ],
   "id": "c97b7a69649eeb63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vcata\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer caricato correttamente!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vcata\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vcata\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-it-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T17:29:25.220209Z",
     "start_time": "2024-11-23T17:24:32.197632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verifica se la GPU è disponibile\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Carica il tokenizer e il modello per la sentiment analysis\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\").to(device)\n",
    "# Funzione per l'analisi del sentiment\n",
    "def analyze_sentiment(translated_text, true_sentiment):\n",
    "    # Tokenizza il testo tradotto per l'analisi del sentiment\n",
    "    inputs = sentiment_tokenizer(translated_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "\n",
    "    # Invia il testo al modello di sentiment analysis\n",
    "    with torch.no_grad():\n",
    "        outputs = sentiment_model(**inputs)\n",
    "\n",
    "    # Ottieni le probabilità di sentiment (classi: 0 = Negative, 1 = No sentiment, 2 = Positive)\n",
    "    sentiment_scores = outputs.logits\n",
    "    sentiment_numeric = torch.argmax(sentiment_scores, dim=1).item()  # Ottieni la classe con la probabilità più alta\n",
    "\n",
    "    # Mappa numeri a etichette\n",
    "    sentiment_map = {\n",
    "        0: \"Negative\",\n",
    "        1: \"No sentiment\",\n",
    "        2: \"Positive\"\n",
    "    }\n",
    "    sentiment = sentiment_map.get(sentiment_numeric, \"Unknown\")  # Default a 'Unknown' per valori imprevisti\n",
    "\n",
    "    return {'sentiment': sentiment, 'trueSentiment': true_sentiment}\n",
    "\n",
    "sentiments4 = [analyze_sentiment(translated_text, true_sentiment) for translated_text, true_sentiment in zip(translated_texts, trueSentiment2)]\n",
    "\n",
    "print(sentiments4)\n",
    "\n",
    "# Estrai le predizioni e i sentimenti veri\n",
    "predicted_sentiments = [item['sentiment'] for item in sentiments4]\n",
    "true_sentiments = list(trueSentiment2)\n",
    "\n",
    "# Calcola l'accuracy\n",
    "accuracy = accuracy_score(true_sentiments, predicted_sentiments)\n",
    "\n",
    "# Stampa il risultato\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n"
   ],
   "id": "7abb11bc2e992bf6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 32\u001B[0m\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentiment\u001B[39m\u001B[38;5;124m'\u001B[39m: sentiment, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrueSentiment\u001B[39m\u001B[38;5;124m'\u001B[39m: true_sentiment}\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m# Applica i modelli separatamente\u001B[39;00m\n\u001B[1;32m---> 32\u001B[0m translated_texts \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[43mtranslate_text_it_to_en\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtexts2\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     33\u001B[0m sentiments4 \u001B[38;5;241m=\u001B[39m [analyze_sentiment(translated_text, true_sentiment) \u001B[38;5;28;01mfor\u001B[39;00m translated_text, true_sentiment \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(translated_texts, trueSentiment2)]\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28mprint\u001B[39m(sentiments4)\n",
      "Cell \u001B[1;32mIn[2], line 32\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentiment\u001B[39m\u001B[38;5;124m'\u001B[39m: sentiment, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrueSentiment\u001B[39m\u001B[38;5;124m'\u001B[39m: true_sentiment}\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m# Applica i modelli separatamente\u001B[39;00m\n\u001B[1;32m---> 32\u001B[0m translated_texts \u001B[38;5;241m=\u001B[39m [\u001B[43mtranslate_text_it_to_en\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m texts2]\n\u001B[0;32m     33\u001B[0m sentiments4 \u001B[38;5;241m=\u001B[39m [analyze_sentiment(translated_text, true_sentiment) \u001B[38;5;28;01mfor\u001B[39;00m translated_text, true_sentiment \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(translated_texts, trueSentiment2)]\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28mprint\u001B[39m(sentiments4)\n",
      "Cell \u001B[1;32mIn[1], line 39\u001B[0m, in \u001B[0;36mtranslate_text_it_to_en\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;66;03m# Invia il testo al modello di traduzione\u001B[39;00m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 39\u001B[0m     translated_ids \u001B[38;5;241m=\u001B[39m \u001B[43mtranslation_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;66;03m# Decodifica la traduzione (da ID a testo)\u001B[39;00m\n\u001B[0;32m     42\u001B[0m translated_text \u001B[38;5;241m=\u001B[39m translation_tokenizer\u001B[38;5;241m.\u001B[39mdecode(translated_ids[\u001B[38;5;241m0\u001B[39m], skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:2246\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[0;32m   2238\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[0;32m   2239\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   2240\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[0;32m   2241\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[0;32m   2242\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   2243\u001B[0m     )\n\u001B[0;32m   2245\u001B[0m     \u001B[38;5;66;03m# 13. run beam sample\u001B[39;00m\n\u001B[1;32m-> 2246\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_beam_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2247\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2248\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeam_scorer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2249\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2250\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2251\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2252\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2253\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2254\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2256\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mGROUP_BEAM_SEARCH:\n\u001B[0;32m   2257\u001B[0m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[0;32m   2258\u001B[0m     beam_scorer \u001B[38;5;241m=\u001B[39m BeamSearchScorer(\n\u001B[0;32m   2259\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m   2260\u001B[0m         num_beams\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2266\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mmax_length,\n\u001B[0;32m   2267\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:3523\u001B[0m, in \u001B[0;36mGenerationMixin._beam_search\u001B[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[0;32m   3520\u001B[0m next_tokens \u001B[38;5;241m=\u001B[39m next_tokens \u001B[38;5;241m%\u001B[39m vocab_size\n\u001B[0;32m   3522\u001B[0m \u001B[38;5;66;03m# stateless\u001B[39;00m\n\u001B[1;32m-> 3523\u001B[0m beam_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mbeam_scorer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3524\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3525\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnext_token_scores\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3526\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnext_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3527\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnext_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3528\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3529\u001B[0m \u001B[43m    \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3530\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeam_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeam_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3531\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoder_prompt_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_prompt_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3532\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3534\u001B[0m beam_scores \u001B[38;5;241m=\u001B[39m beam_outputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext_beam_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   3535\u001B[0m beam_next_tokens \u001B[38;5;241m=\u001B[39m beam_outputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext_beam_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\beam_search.py:298\u001B[0m, in \u001B[0;36mBeamSearchScorer.process\u001B[1;34m(self, input_ids, next_scores, next_tokens, next_indices, pad_token_id, eos_token_id, beam_indices, group_index, decoder_prompt_len)\u001B[0m\n\u001B[0;32m    295\u001B[0m         beam_idx \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    297\u001B[0m     \u001B[38;5;66;03m# once the beam for next step is full, don't add more tokens to it.\u001B[39;00m\n\u001B[1;32m--> 298\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m beam_idx \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroup_size:\n\u001B[0;32m    299\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m beam_idx \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroup_size:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7871ed72a230e87a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Verifica se CUDA è disponibile\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Mostra il numero di GPU disponibili (se CUDA è disponibile)\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "# Nome della GPU in uso (se CUDA è disponibile)\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA non è disponibile\")\n"
   ],
   "id": "d83f38b834d39ee6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "80687db574df38a1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
